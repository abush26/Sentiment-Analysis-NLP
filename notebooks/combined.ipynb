{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import emoji\n",
    "import sentencepiece as spm\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l2\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import fasttext\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, BatchNormalization, Conv1D, MaxPooling1D, LSTM, SimpleRNN, Bidirectional, Dense, GlobalMaxPooling1D, Dropout,GRU, Input, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/70000correctly labled data.csv\")\n",
    "df = df[(df.sentiment != \"Neutral\") & (df.sentiment != \"labels\")]\n",
    "df.rename(columns={\"sentiment\":\"labels\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Positive', 'Negative', nan], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.labels.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive    24948\n",
       "Negative    20728\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_redu = df.copy()\n",
    "df_redu = df_redu.sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = set(\n",
    "    \"\"\"\n",
    "ግን አንቺ አንተ እናንተ ያንተ ያንቺ የናንተ ራስህን ራስሽን ራሳችሁን\n",
    "ሁሉ ኋላ በሰሞኑ አሉ በኋላ ሁኔታ በኩል አስታውቀዋል ሆነ በውስጥ\n",
    "አስታውሰዋል ሆኑ ባጣም እስካሁን ሆኖም በተለይ አሳሰበ ሁል በተመለከተ\n",
    "አሳስበዋል ላይ በተመሳሳይ አስፈላጊ ሌላ የተለያየ አስገነዘቡ ሌሎች የተለያዩ\n",
    "አስገንዝበዋል ልዩ ተባለ አብራርተዋል መሆኑ ተገለጸ አስረድተዋል  ተገልጿል\n",
    "ማለቱ ተጨማሪ እባክህ የሚገኝ ተከናወነ እባክሽ ማድረግ ችግር አንጻር ማን\n",
    "ትናንት እስኪደርስ ነበረች እንኳ ሰሞኑን ነበሩ እንኳን ሲሆን ነበር እዚሁ ሲል\n",
    "ነው እንደገለጹት አለ ና እንደተናገሩት ቢሆን ነገር እንዳስረዱት ብለዋል ነገሮች\n",
    "እንደገና ብዙ ናት ወቅት ቦታ ናቸው እንዲሁም በርካታ አሁን እንጂ እስከ\n",
    "ማለት የሚሆኑት ስለማናቸውም ውስጥ ይሆናሉ ሲባል ከሆነው ስለዚሁ ከአንድ\n",
    "ያልሆነ ሳለ የነበረውን ከአንዳንድ በማናቸውም በሙሉ የሆነው ያሉ በእነዚሁ\n",
    "ወር መሆናቸው ከሌሎች በዋና አንዲት ወይም\n",
    "በላይ እንደ በማቀድ ለሌሎች በሆኑ ቢሆንም ጊዜና  ይሆኑበታል በሆነ አንዱ\n",
    "ለዚህ ለሆነው ለነዚህ ከዚህ የሌላውን ሶስተኛ አንዳንድ ለማንኛውም የሆነ ከሁለት\n",
    "የነገሩ ሰኣት አንደኛ እንዲሆን እንደነዚህ ማንኛውም ካልሆነ የሆኑት  ጋር ቢያንስ\n",
    "ይህንንም እነደሆነ እነዚህን ይኸው  የማናቸውም\n",
    "በሙሉም ይህችው በተለይም አንዱን የሚችለውን በነዚህ ከእነዚህ በሌላ\n",
    "የዚሁ ከእነዚሁ ለዚሁ በሚገባ ለእያንዳንዱ የአንቀጹ ወደ ይህም ስለሆነ ወይ\n",
    "ማናቸውንም ተብሎ እነዚህ መሆናቸውን የሆነችን ከአስር ሳይሆን ከዚያ የለውም\n",
    "የማይበልጥ እንደሆነና እንዲሆኑ  በሚችሉ ብቻ ብሎ ከሌላ የሌላቸውን\n",
    "ለሆነ በሌሎች ሁለቱንም በቀር ይህ በታች አንደሆነ በነሱ\n",
    "ይህን የሌላ እንዲህ ከሆነ ያላቸው በነዚሁ በሚል የዚህ ይህንኑ\n",
    "በእንደዚህ ቁጥር ማናቸውም ሆነው ባሉ በዚህ በስተቀር ሲሆንና\n",
    "በዚህም መሆን ምንጊዜም እነዚህም በዚህና ያለ ስም\n",
    "ሲኖር ከዚህም መሆኑን በሁኔታው የማያንስ እነዚህኑ ማንም ከነዚሁ\n",
    "ያላቸውን እጅግ ሲሆኑ ለሆኑ ሊሆን ለማናቸውም እና ነዉ\n",
    "\"\"\".split()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_redu.texts.values\n",
    "labels = df_redu.labels.values\n",
    "encoded_labels = []\n",
    "for label in labels:\n",
    "    if label == \"Negative\":\n",
    "        encoded_labels.append(0)\n",
    "    else:\n",
    "        encoded_labels.append(1)\n",
    "y = np.array(encoded_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "##### Steps Taken\n",
    "- filter stop wors\n",
    "- remove emojis\n",
    "- remove punc and special chars\n",
    "- remove ascii and numbers\n",
    "- normalize mogisha characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stop_words(ls):\n",
    "    new_ls_stop = []\n",
    "    for input in ls:\n",
    "        cleaned = [w for w in input.split(\" \") if not w in STOP_WORDS]\n",
    "        new_ls_stop.append(\" \".join(cleaned).strip())\n",
    "\n",
    "    return new_ls_stop\n",
    "    \n",
    "def remove_emojis(ls):\n",
    "    new_ls = []\n",
    "    for input in ls:\n",
    "        new_ls.append(emoji.replace_emoji(input)\n",
    ")\n",
    "    return new_ls\n",
    "    \n",
    "def remove_punc_and_special_chars(ls): \n",
    "    new_ls = []\n",
    "    for text in ls:\n",
    "        text = str(text)\n",
    "        text = re.sub('[፣]', ' ',text)\n",
    "        normalized_text = re.sub('[\\!\\@\\#\\$\\%\\^\\«\\»\\&\\*\\(\\)\\…\\[\\]\\{\\}\\;\\“\\”\\›\\’\\‘\\\"\\'\\:\\,\\.\\‹\\/\\<\\>\\?\\\\\\\\|\\`\\´\\~\\-\\=\\+\\፡\\።\\፤\\;\\፦\\፥\\፧\\፨\\፠\\፣]', '',text)\n",
    "        new_ls.append(normalized_text)\n",
    "    return new_ls\n",
    "\n",
    "def remove_ascii_and_numbers(ls):\n",
    "    new_ls = []\n",
    "    for text_input in ls:\n",
    "        text_input = str(text_input)\n",
    "        rm_num_and_ascii=re.sub('[A-Za-z0-9]','',text_input)\n",
    "        text = re.sub('[\\'\\u1369-\\u137C\\']+','',rm_num_and_ascii)\n",
    "        new_ls.append(text)\n",
    "    return new_ls\n",
    "\n",
    "def normalize_char_level_missmatch(ls):\n",
    "        new_ls = []\n",
    "        for input_token in ls:\n",
    "                input_token = str(input_token)\n",
    "                rep1=re.sub('[ሃኅኃሐሓኻ]','ሀ',input_token)\n",
    "                rep2=re.sub('[ሑኁዅ]','ሁ',rep1)\n",
    "                rep3=re.sub('[ኂሒኺ]','ሂ',rep2)\n",
    "                rep4=re.sub('[ኌሔዄ]','ሄ',rep3)\n",
    "                rep5=re.sub('[ሕኅ]','ህ',rep4)\n",
    "                rep6=re.sub('[ኆሖኾ]','ሆ',rep5)\n",
    "                rep7=re.sub('[ሠ]','ሰ',rep6)\n",
    "                rep8=re.sub('[ሡ]','ሱ',rep7)\n",
    "                rep9=re.sub('[ሢ]','ሲ',rep8)\n",
    "                rep10=re.sub('[ሣ]','ሳ',rep9)\n",
    "                rep11=re.sub('[ሤ]','ሴ',rep10)\n",
    "                rep12=re.sub('[ሥ]','ስ',rep11)\n",
    "                rep13=re.sub('[ሦ]','ሶ',rep12)\n",
    "                rep14=re.sub('[ዓኣዐ]','አ',rep13)\n",
    "                rep15=re.sub('[ዑ]','ኡ',rep14)\n",
    "                rep16=re.sub('[ዒ]','ኢ',rep15)\n",
    "                rep17=re.sub('[ዔ]','ኤ',rep16)\n",
    "                rep18=re.sub('[ዕ]','እ',rep17)\n",
    "                rep19=re.sub('[ዖ]','ኦ',rep18)\n",
    "                rep20=re.sub('[ጸ]','ፀ',rep19)\n",
    "                rep21=re.sub('[ጹ]','ፁ',rep20)\n",
    "                rep22=re.sub('[ጺ]','ፂ',rep21)\n",
    "                rep23=re.sub('[ጻ]','ፃ',rep22)\n",
    "                rep24=re.sub('[ጼ]','ፄ',rep23)\n",
    "                rep25=re.sub('[ጽ]','ፅ',rep24)\n",
    "                rep26=re.sub('[ጾ]','ፆ',rep25)\n",
    "                #Normalizing words with Labialized Amharic characters such as በልቱዋል or  በልቱአል to  በልቷል  \n",
    "                rep27=re.sub('(ሉ[ዋአ])','ሏ',rep26)\n",
    "                rep28=re.sub('(ሙ[ዋአ])','ሟ',rep27)\n",
    "                rep29=re.sub('(ቱ[ዋአ])','ቷ',rep28)\n",
    "                rep30=re.sub('(ሩ[ዋአ])','ሯ',rep29)\n",
    "                rep31=re.sub('(ሱ[ዋአ])','ሷ',rep30)\n",
    "                rep32=re.sub('(ሹ[ዋአ])','ሿ',rep31)\n",
    "                rep33=re.sub('(ቁ[ዋአ])','ቋ',rep32)\n",
    "                rep34=re.sub('(ቡ[ዋአ])','ቧ',rep33)\n",
    "                rep35=re.sub('(ቹ[ዋአ])','ቿ',rep34)\n",
    "                rep36=re.sub('(ሁ[ዋአ])','ኋ',rep35)\n",
    "                rep37=re.sub('(ኑ[ዋአ])','ኗ',rep36)\n",
    "                rep38=re.sub('(ኙ[ዋአ])','ኟ',rep37)\n",
    "                rep39=re.sub('(ኩ[ዋአ])','ኳ',rep38)\n",
    "                rep40=re.sub('(ዙ[ዋአ])','ዟ',rep39)\n",
    "                rep41=re.sub('(ጉ[ዋአ])','ጓ',rep40)\n",
    "                rep42=re.sub('(ደ[ዋአ])','ዷ',rep41)\n",
    "                rep43=re.sub('(ጡ[ዋአ])','ጧ',rep42)\n",
    "                rep44=re.sub('(ጩ[ዋአ])','ጯ',rep43)\n",
    "                rep45=re.sub('(ጹ[ዋአ])','ጿ',rep44)\n",
    "                rep46=re.sub('(ፉ[ዋአ])','ፏ',rep45)\n",
    "                rep47=re.sub('[ቊ]','ቁ',rep46) #ቁ can be written as ቊ\n",
    "                rep48=re.sub('[ኵ]','ኩ',rep47) #ኩ can be also written as ኵ  \n",
    "                new_ls.append(rep48)\n",
    "        return new_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46553"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = normalize_char_level_missmatch(X)\n",
    "X = remove_punc_and_special_chars(X)\n",
    "X = remove_ascii_and_numbers(X)\n",
    "X = remove_emojis(X)\n",
    "X = filter_stop_words(X)\n",
    "\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spliting the data to 80/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenizing the data (this is a very important stage and depends on the tokeinzer especiall for a langugage in amharic)\n",
    "- padding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = True\n",
    "if token == True:\n",
    "    tokenizer = Tokenizer(num_words=50000)\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "    X_train = tokenizer.texts_to_sequences(X_train)\n",
    "    X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "else:\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.Load(\"../embeddings/amh_sp.model\")\n",
    "\n",
    "    # sp.EncodeAsPieces(X[1])\n",
    "    X_train = sp.EncodeAsIds(X_train)\n",
    "    X_test = sp.EncodeAsIds(X_test)\n",
    "\n",
    "    vocab_size = sp.GetPieceSize() + 1\n",
    "\n",
    "\n",
    "maxlen = 10\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37242"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37242"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[:40000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepartion for Trainning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implement early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # monitor both validation loss and validation accuracy\n",
    "    mode='min',          # stop training when the quantity monitored has stopped decreasing\n",
    "    patience=5,          # number of epochs with no improvement after which training will be stopped\n",
    "    verbose=1,\n",
    "    restore_best_weights=True  # restore the weights from the epoch with the best value of the monitored quantity\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- adding embeddings using fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "## fast text word embedding\n",
    "word_vectors = fasttext.load_model(\"cc.am.100.bin\")\n",
    "\n",
    "# Define the maximum number of words to keep\n",
    "max_words = 50000\n",
    "\n",
    "# Extract the embedding matrix from the FastText model\n",
    "embedding_dim = word_vectors.get_dimension()\n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(max_words, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_words:\n",
    "        continue\n",
    "    if word in word_vectors:\n",
    "        embedding_matrix[i] = word_vectors[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-20 16:18:36.067013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-20 16:18:36.178151: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /u01/app/oracle/product/11.2.0/xe/lib:\n",
      "2023-05-20 16:18:36.178191: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-05-20 16:18:36.180967: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-20 16:18:36.206268: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 20000400 exceeds 10% of free system memory.\n",
      "2023-05-20 16:18:36.217455: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 20000400 exceeds 10% of free system memory.\n",
      "2023-05-20 16:18:36.221106: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 20000400 exceeds 10% of free system memory.\n",
      "2023-05-20 16:18:36.238111: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 20000400 exceeds 10% of free system memory.\n",
      "2023-05-20 16:18:36.290430: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 20000400 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the deep learning models\n",
    "models = [\n",
    "    {\n",
    "        'name': 'CNN',\n",
    "        'model': Sequential([\n",
    "            (Embedding(max_words + 1,\n",
    "                    embedding_dim,\n",
    "                    embeddings_initializer='glorot_uniform',\n",
    "                    input_length=maxlen,\n",
    "                    weights=[embedding_matrix],\n",
    "                    trainable=False)),\n",
    "            Conv1D(32, 5, activation='relu'),\n",
    "            GlobalMaxPooling1D(),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(0.5),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "    },\n",
    "    {\n",
    "        'name': 'RNN',\n",
    "        'model': Sequential([\n",
    "            (Embedding(max_words + 1,\n",
    "                    embedding_dim,\n",
    "                    embeddings_initializer='glorot_uniform',\n",
    "                    input_length=maxlen,\n",
    "                    weights=[embedding_matrix],\n",
    "                    trainable=False)),\n",
    "            SimpleRNN(32, kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dropout(0.5),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "    },\n",
    "    {\n",
    "        'name': 'LSTM',\n",
    "        'model': Sequential([\n",
    "            (Embedding(max_words + 1,\n",
    "                    embedding_dim,\n",
    "                    embeddings_initializer='glorot_uniform',\n",
    "                    input_length=maxlen,\n",
    "                    weights=[embedding_matrix],\n",
    "                    trainable=False)),\n",
    "            LSTM(32, activation='relu'),\n",
    "            Dense(64, activation='sigmoid'),\n",
    "            Dropout(0.5),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(1, activation='sigmoid')\n",
    "            \n",
    "        ])\n",
    "    },\n",
    "    {\n",
    "        'name': 'bi-LSTM',\n",
    "        'model': Sequential([\n",
    "            (Embedding(max_words + 1,\n",
    "                    embedding_dim,\n",
    "                    embeddings_initializer='glorot_uniform',\n",
    "                    input_length=maxlen,\n",
    "                    weights=[embedding_matrix],\n",
    "                    trainable=False)),\n",
    "            Bidirectional(LSTM(32, kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01), dropout=0.2, recurrent_dropout=0.2)),\n",
    "            Dense(32, activation='sigmoid'),\n",
    "            Dropout(0.5),\n",
    "            Dense(1, activation='sigmoid')\n",
    "            \n",
    "        ])\n",
    "    },\n",
    "    {\n",
    "        'name': 'GRU',\n",
    "        'model': Sequential([\n",
    "            (Embedding(max_words + 1,\n",
    "                    embedding_dim,\n",
    "                    embeddings_initializer='glorot_uniform',\n",
    "                    input_length=maxlen,\n",
    "                    weights=[embedding_matrix],\n",
    "                    trainable=False)),\n",
    "            GRU(units=32, dropout=0.2, recurrent_dropout=0.2),\n",
    "            Dense(1, activation='sigmoid')           \n",
    "        ])\n",
    "    }\n",
    "\n",
    "   \n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile each model\n",
    "for m in models:\n",
    "    m['model'].compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Train each model with varying amounts of data\u001b[39;00m\n\u001b[32m      2\u001b[39m train_sizes = [\u001b[32m5000\u001b[39m, \u001b[32m10000\u001b[39m, \u001b[32m15000\u001b[39m, \u001b[32m20000\u001b[39m, \u001b[32m25000\u001b[39m, \u001b[32m30000\u001b[39m, \u001b[32m38000\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m histories = {m[\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m]: [] \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodels\u001b[49m}\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m size \u001b[38;5;129;01min\u001b[39;00m train_sizes:\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m models:\n",
      "\u001b[31mNameError\u001b[39m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "# Train each model with varying amounts of data\n",
    "train_sizes = [5000, 10000, 15000, 20000, 25000, 30000, 38000]\n",
    "histories = {m['name']: [] for m in models}\n",
    "for size in train_sizes:\n",
    "    for m in models:\n",
    "        history = m['model'].fit(X_train[:size], y_train[:size], epochs=80, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "        histories[m['name']].append(history)\n",
    "\n",
    "# Plot the accuracy as a function of training data size for each model\n",
    "fig, ax = plt.subplots()\n",
    "for m in models:\n",
    "    accuracies = []\n",
    "    for i in range(len(train_sizes)):\n",
    "        accuracy = histories[m['name']][i].history['val_accuracy'][-1]\n",
    "        accuracies.append(accuracy)\n",
    "    ax.plot(train_sizes, accuracies, label=m['name'])\n",
    "ax.set_xlabel('Data Size')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Accuracy as a function of training data size for different deep learning models')\n",
    "ax.legend()\n",
    "plt.savefig(\"../fig/unified_accuracy_2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
